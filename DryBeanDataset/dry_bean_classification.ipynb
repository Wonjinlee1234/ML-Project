{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7ffac7-ff53-4eda-ba96-0e052429d816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRatio  \\\n",
      "0  28395    610.291       208.178117       173.888747     1.197191   \n",
      "1  28734    638.018       200.524796       182.734419     1.097356   \n",
      "2  29380    624.110       212.826130       175.931143     1.209713   \n",
      "3  30008    645.884       210.557999       182.516516     1.153638   \n",
      "4  30140    620.134       201.847882       190.279279     1.060798   \n",
      "\n",
      "   Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  Roundness  \\\n",
      "0      0.549812       28715     190.141097  0.763923  0.988856   0.958027   \n",
      "1      0.411785       29172     191.272751  0.783968  0.984986   0.887034   \n",
      "2      0.562727       29690     193.410904  0.778113  0.989559   0.947849   \n",
      "3      0.498616       30724     195.467062  0.782681  0.976696   0.903936   \n",
      "4      0.333680       30417     195.896503  0.773098  0.990893   0.984877   \n",
      "\n",
      "   Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  \n",
      "0     0.913358      0.007332      0.003147      0.834222      0.998724  \n",
      "1     0.953861      0.006979      0.003564      0.909851      0.998430  \n",
      "2     0.908774      0.007244      0.003048      0.825871      0.999066  \n",
      "3     0.928329      0.007017      0.003215      0.861794      0.994199  \n",
      "4     0.970516      0.006697      0.003665      0.941900      0.999166  \n",
      "   Class\n",
      "0  SEKER\n",
      "1  SEKER\n",
      "2  SEKER\n",
      "3  SEKER\n",
      "4  SEKER\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "dry_bean = fetch_ucirepo(id=602) \n",
    "  \n",
    "# X shows the input datas for each of the beans. Whearas Y tells the output, name of the bean\n",
    "X = dry_bean.data.features \n",
    "y = dry_bean.data.targets \n",
    "\n",
    "#shows how the data structure looks\n",
    "print(X[:5])\n",
    "print(y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5226fd-77e2-4d60-bf49-f8e3f3776b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled features shape: [-1.30659814 -1.39591111 -1.25235661 ... -0.45047814 -0.42897404\n",
      " -0.2917356 ]\n",
      "One-hot labels shape: [0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing of the data for later machine learning\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Flattens to 1D array\n",
    "y = np.array(y).ravel()\n",
    "\n",
    "# Encodes the labels from string to int\n",
    "label_encoder = LabelEncoder()\n",
    "y_int = label_encoder.fit_transform(y)\n",
    "\n",
    "# makes 1 to 1 0 0 0 0 0 for hot encoding\n",
    "y_int = y_int.reshape(-1, 1) \n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)  \n",
    "y_onehot = onehot_encoder.fit_transform(y_int)\n",
    "\n",
    "#unit balance, centered at zero, makes the machine learning faster\n",
    "def normalization(x):\n",
    "    X = np.array(x, dtype=float)\n",
    "    X_norm = np.zeros_like(X)\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        inputX = X[:, i]\n",
    "        X_norm[:, i] = (inputX - np.mean(inputX)) / np.std(inputX)\n",
    "    \n",
    "    return X_norm\n",
    "\n",
    "X_scaled = normalization(X)\n",
    "\n",
    "\n",
    "print(\"Scaled features shape:\", X_scaled[:,2])\n",
    "print(\"One-hot labels shape:\", y_onehot[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f75118-1abf-477a-937b-f2bde50387ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 10888\n",
      "Testing samples: 2723\n"
     ]
    }
   ],
   "source": [
    "# splittig the data to training group and test group\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_onehot, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Testing samples:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e210f1-a7ea-425c-9453-6f1ddb1ec1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial W (zeros): 1.946\n"
     ]
    }
   ],
   "source": [
    "#defining functions for the learning\n",
    "# Sigmoid can not be used for multivariate logistic training, softmax is used for activation function instead all the functions from lab2/ 4\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "def compute_cost_multiclass(X, y, W, b):\n",
    "    m = X.shape[0]\n",
    "    total_cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], W) + b  \n",
    "        f_wb_i = softmax(z_i)      \n",
    "        total_cost += -np.sum(y[i] * np.log(f_wb_i))\n",
    "    \n",
    "    total_cost /= m\n",
    "    return total_cost\n",
    "\n",
    "num_classes = y_train.shape[1]\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "W = np.zeros((num_features, num_classes))\n",
    "b = np.zeros(num_classes)\n",
    "\n",
    "cost = compute_cost_multiclass(X_train, y_train, W, b)\n",
    "print('Cost at initial W (zeros): {:.3f}'.format(cost))\n",
    "\n",
    "def compute_gradient_multiclass(X, y, W, b, regression, lambda_):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    #shows how many neurons there are, for us it is 7\n",
    "    num_classes = y.shape[1]\n",
    "    \n",
    "    dj_dw = np.zeros_like(W) \n",
    "    dj_db = np.zeros_like(b)  \n",
    "    \n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], W) + b  \n",
    "        f_wb = softmax(z_i)               \n",
    "        diff = f_wb - y[i]                 \n",
    "        \n",
    "        dj_dw += np.outer(X[i], diff)       \n",
    "        dj_db += diff                        \n",
    "    \n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "    if regression:\n",
    "        dj_dw += (lambda_ / m) * W\n",
    "\n",
    "    return dj_db, dj_dw\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def gradient_descent_multiclass(X, y, W_in, b_in, cost_function, gradient_function, alpha, num_iters, regression, lambda_): \n",
    "    \n",
    "    J_history = []\n",
    "    W_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Compute gradients\n",
    "        dj_db, dj_dw = gradient_function(X, y, W_in, b_in, regression, lambda_)\n",
    "        \n",
    "        # Update parameters\n",
    "        W_in = W_in - alpha * dj_dw\n",
    "        b_in = b_in - alpha * dj_db\n",
    "        \n",
    "        # Compute and save cost\n",
    "        if i < 100000: \n",
    "            cost = cost_function(X, y, W_in, b_in)\n",
    "            J_history.append(cost)\n",
    "\n",
    "\n",
    "        if i % max(1, math.ceil(num_iters / 10)) == 0 or i == (num_iters - 1):\n",
    "            W_history.append(W_in.copy())  \n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.3f}\")\n",
    "    \n",
    "    return W_in, b_in, J_history, W_history\n",
    "\n",
    "def predict_multiclass(X, W, b):\n",
    "    \n",
    "    Z = np.dot(X, W) + b  \n",
    "    exp_Z = np.exp(Z)  \n",
    "    probs = exp_Z / np.sum(exp_Z)\n",
    "    p = np.argmax(probs, axis=1)  \n",
    "    return p\n",
    "\n",
    "#Regularization functions (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a5f81-f885-42bf-9013-8181614d6f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost    1.899\n",
      "Iteration   40: Cost    1.147\n",
      "Iteration   80: Cost    0.932\n",
      "Iteration  120: Cost    0.817\n",
      "Iteration  160: Cost    0.741\n",
      "Iteration  200: Cost    0.683\n",
      "Iteration  240: Cost    0.638\n",
      "Iteration  280: Cost    0.601\n",
      "Iteration  320: Cost    0.571\n",
      "Iteration  360: Cost    0.545\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# first randomly assigns W and b values for the training\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "W_init = 0.01 * (np.random.rand(num_features, num_classes) - 0.5)\n",
    "b_init = 0.01 * (np.random.rand(num_classes) - 0.5)\n",
    "\n",
    "regression = False\n",
    "iterations = 400  \n",
    "alpha = 0.03      \n",
    "lambda_ = 0.001\n",
    "\n",
    "#train\n",
    "W_final, b_final, J_history, W_history = gradient_descent_multiclass(\n",
    "    X_train, y_train, W_init, b_init,\n",
    "    compute_cost_multiclass,\n",
    "    compute_gradient_multiclass,\n",
    "    alpha,\n",
    "    iterations,\n",
    "    regression,\n",
    "    lambda_\n",
    ")\n",
    "\n",
    "print(\"Final cost after gradient descent:\", J_history[-1])\n",
    "\n",
    "y_tested = predict_multiclass(X_test, W_final, b_final)\n",
    "\n",
    "bean_classes = ['SEKER', 'BARBUNYA', 'BOMBAY', 'CALI', 'DERMOSAN', 'HOROZ', 'SIRA']\n",
    "predicted_beans = [bean_classes[i] for i in y_tested]\n",
    "for i in range(10):\n",
    "    print(f\"Predicted bean: {predicted_beans[i]}\")\n",
    "\n",
    "# Accurracy\n",
    "# finds the maximum value from the list meaning that in the hot encoding 001000, shows which bean is selected\n",
    "y_true_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(y_true_test == y_true_test)\n",
    "print(\"Test set accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba27b25-d201-42ed-9270-5c58404b5211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial W (zeros): 1.946\n"
     ]
    }
   ],
   "source": [
    "# defining functions for the learning\n",
    "# Softmax for multiclass\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "# --------- COST WITH L2 REGULARIZATION ----------\n",
    "def compute_cost_multiclass_reg(X, y, W, b, lambda_):\n",
    "    m = X.shape[0]\n",
    "    total_cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], W) + b  \n",
    "        f_wb_i = softmax(z_i)\n",
    "        total_cost += -np.sum(y[i] * np.log(f_wb_i))\n",
    "\n",
    "    total_cost /= m\n",
    "\n",
    "    # L2 penalty\n",
    "    reg_cost = (lambda_ / (2 * m)) * np.sum(W * W)\n",
    "\n",
    "    return total_cost + reg_cost\n",
    "\n",
    "# initialize W, b\n",
    "num_classes = y_train.shape[1]\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "W = np.zeros((num_features, num_classes))\n",
    "b = np.zeros(num_classes)\n",
    "\n",
    "# Example cost at initialization\n",
    "cost = compute_cost_multiclass_reg(X_train, y_train, W, b, lambda_=0.1)\n",
    "print('Cost at initial W (zeros): {:.3f}'.format(cost))\n",
    "\n",
    "# --------- GRADIENT WITH L2 REGULARIZATION ----------\n",
    "def compute_gradient_multiclass_reg(X, y, W, b, lambda_):\n",
    "    m, n = X.shape\n",
    "    num_classes = y.shape[1]\n",
    "    \n",
    "    dj_dw = np.zeros_like(W)\n",
    "    dj_db = np.zeros_like(b)\n",
    "    \n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], W) + b\n",
    "        f_wb = softmax(z_i)\n",
    "        diff = f_wb - y[i]\n",
    "        \n",
    "        dj_dw += np.outer(X[i], diff)\n",
    "        dj_db += diff\n",
    "    \n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "\n",
    "    # L2 gradient (no bias)\n",
    "    dj_dw += (lambda_ / m) * W\n",
    "    \n",
    "    return dj_db, dj_dw\n",
    "\n",
    "# --------- REGULARIZED GRADIENT DESCENT ----------\n",
    "def gradient_descent_multiclass(X, y, W, b, alpha, num_iters, lambda_):\n",
    "    \n",
    "    J_history = []\n",
    "    W_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        dj_db, dj_dw = compute_gradient_multiclass_reg(X, y, W, b, lambda_)\n",
    "\n",
    "        W = W - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        cost = compute_cost_multiclass_reg(X, y, W, b, lambda_)\n",
    "        J_history.append(cost)\n",
    "\n",
    "        if i % max(1, num_iters // 10) == 0:\n",
    "            W_history.append(W.copy())\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.3f}\")\n",
    "    \n",
    "    return W, b, J_history, W_history\n",
    "\n",
    "\n",
    "def predict_multiclass(X, W, b):\n",
    "    Z = np.dot(X, W) + b  \n",
    "    exp_Z = np.exp(Z)  \n",
    "    probs = exp_Z / np.sum(exp_Z)\n",
    "    p = np.argmax(probs, axis=1)  \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3d203a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost    1.899\n",
      "Iteration   40: Cost    1.147\n",
      "Iteration   80: Cost    0.932\n",
      "Iteration  120: Cost    0.818\n",
      "Iteration  160: Cost    0.741\n",
      "Iteration  200: Cost    0.683\n",
      "Iteration  240: Cost    0.638\n",
      "Iteration  280: Cost    0.601\n",
      "Iteration  320: Cost    0.571\n",
      "Iteration  360: Cost    0.545\n",
      "Final cost after gradient descent: 0.5225933574393123\n",
      "Predicted bean: CALI\n",
      "Predicted bean: BARBUNYA\n",
      "Predicted bean: SIRA\n",
      "Predicted bean: HOROZ\n",
      "Predicted bean: SEKER\n",
      "Predicted bean: HOROZ\n",
      "Predicted bean: CALI\n",
      "Predicted bean: SIRA\n",
      "Predicted bean: BOMBAY\n",
      "Predicted bean: DERMOSAN\n",
      "Test set accuracy: 0.8806463459419758\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# first randomly assigns W and b values for the training\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "W_init = 0.01 * (np.random.rand(num_features, num_classes) - 0.5)\n",
    "b_init = 0.01 * (np.random.rand(num_classes) - 0.5)\n",
    "\n",
    "iterations = 400  \n",
    "alpha = 0.03      \n",
    "lambda_ = 0.1     # choose any Î» you want\n",
    "\n",
    "# ---- train model with REGULARIZATION ----\n",
    "W_final, b_final, J_history, W_history = gradient_descent_multiclass(\n",
    "    X_train, y_train, W_init, b_init,\n",
    "    alpha=alpha,\n",
    "    num_iters=iterations,\n",
    "    lambda_=lambda_\n",
    ")\n",
    "\n",
    "print(\"Final cost after gradient descent:\", J_history[-1])\n",
    "\n",
    "\n",
    "y_tested = predict_multiclass(X_test, W_final, b_final)\n",
    "\n",
    "y_true_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(y_tested == y_true_test)\n",
    "print(\"Test set accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rd_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
